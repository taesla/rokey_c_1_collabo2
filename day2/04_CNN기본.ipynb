{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nlKU-bltjBv"
      },
      "source": [
        "# 복습하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVcxJr4QAebp"
      },
      "source": [
        "## Computer vision의 사용 예"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3cyOg6zAhWf"
      },
      "source": [
        "<img src=\"https://github.com/user-attachments/assets/6e1be390-77fe-4b6e-b7aa-d1f4ef43f0d2\" width=\"1000\">\n",
        "\n",
        "출처: https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAsELklNd4TM"
      },
      "source": [
        "## Neural Network\n",
        "\n",
        "![image](https://github.com/user-attachments/assets/205506a5-45fe-4f5a-b3a3-340a4a81eb69)\n",
        "\n",
        "출처:https://towardsdatascience.com/the-concept-of-artificial-neurons-perceptrons-in-neural-networks-fab22249cbfc\n",
        "\n",
        "\n",
        "| 특성          | 생물학적 뉴런                          | 인공 신경망                           |\n",
        "|---------------|---------------------------------------|---------------------------------------|\n",
        "| **구조**  | 수상돌기, 세포핵, 축삭          | 입력 노드, 은닉층, 출력 노드          |\n",
        "| **신호 전달** | 전기적 신호와 화학적 신호 전달          | 수학적 함수와 가중치를 통한 정보 전달 |\n",
        "| **학습 방식** | 시냅스 가소성을 통한 학습              | 손실 함수 최소화를 통한 학습(역전파) |\n",
        "| **정보 저장** | 시냅스 강도 변화                       | 가중치 및 편향                       |\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/12128784/140010649-40dce697-5e93-4b64-ab3a-3c547c932ad1.png)\n",
        "\n",
        "\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/12128784/111853348-004e6180-895e-11eb-9fd9-8ebfca052b0a.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rkw5A3swtcsK"
      },
      "source": [
        "\n",
        "![image](https://github.com/user-attachments/assets/f0ce59f2-a7ae-4043-b834-28d0586872b0)\n",
        "\n",
        "\n",
        "![image](https://github.com/user-attachments/assets/73a3d9f8-2665-4d46-8071-5aba935849e8)\n",
        "\n",
        "출처: https://medium.com/@Suraj_Yadav/in-depth-knowledge-of-convolutional-neural-networks-b4bfff8145ab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipzg20W9ddQK"
      },
      "source": [
        "## FC-layer sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBpIHcDZCA8B"
      },
      "source": [
        "![image](https://user-images.githubusercontent.com/12128784/140010649-40dce697-5e93-4b64-ab3a-3c547c932ad1.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMrYfo75dgqZ",
        "outputId": "01b3572c-6a57-420a-8d2a-4c7681929e74"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make Tensor 10,10. dtype=int\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m fc1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      6\u001b[0m fc1\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m],\n\u001b[0;32m      7\u001b[0m                                 [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.1\u001b[39m],\n\u001b[0;32m      8\u001b[0m                                 [\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# Make Tensor 10,10. dtype=int\n",
        "import torch\n",
        "\n",
        "fc1 = torch.nn.Linear(4, 3)\n",
        "\n",
        "fc1.weight.data = torch.tensor([[0.1, 0.3, 0.1, 0.1],\n",
        "                                [0.1, 0.3, 0.4, 0.1],\n",
        "                                [0.3, 0.3, 0.1, 0.2]], dtype=torch.float)\n",
        "fc1.bias.data = torch.tensor([3, 6, 9], dtype=torch.float)\n",
        "\n",
        "\n",
        "input = torch.tensor([1, 2, 3, 4], dtype=torch.int).float()\n",
        "print(input)\n",
        "print(\"=\" * 100)\n",
        "print(fc1.weight.data)\n",
        "print(fc1.bias.data)\n",
        "print(\"=\" * 100)\n",
        "output = fc1(input)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bvhX60DEeff"
      },
      "source": [
        "# CNN(Convolution Neural Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZQLIce_GxP0"
      },
      "source": [
        "![ezgif com-animated-gif-maker](https://github.com/user-attachments/assets/f726212f-95f6-4408-9cc4-62da36c38086)\n",
        "\n",
        "출처: https://www.youtube.com/watch?v=pj9-rr1wDhM&t=336s\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16-rNvOAHGl4"
      },
      "source": [
        "![image](https://github.com/user-attachments/assets/1203400f-92ee-4561-995b-a48c42007178)\n",
        "\n",
        "출처: https://medium.com/@Suraj_Yadav/in-depth-knowledge-of-convolutional-neural-networks-b4bfff8145ab\n",
        "\n",
        "0 * 0 + -1 * 0 + 0 * 0 +\n",
        "\n",
        "0 * -1 + 60 * 5 + 113 * -1\n",
        "\n",
        "0 * 0 + 73 * -1 + 121 * 0 = 114"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4LtujiaID12"
      },
      "source": [
        "![1_O06nY1U7zoP4vE5AZEnxKA](https://github.com/user-attachments/assets/7238bdaa-65ad-458a-84a7-b16ab8a4362e)\n",
        "\n",
        "출처: https://medium.com/@Suraj_Yadav/in-depth-knowledge-of-convolutional-neural-networks-b4bfff8145ab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjHzkue4INuW"
      },
      "source": [
        "## Stride"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBWHM9o0IcBd"
      },
      "source": [
        "![1_Bmx3AHBhx0D5OmBSxtY6gw](https://github.com/user-attachments/assets/868b19c8-62c3-4b2b-94c2-4b472445924f)\n",
        "\n",
        "![1_X22-wmPcir4y5VoeqDyvWg](https://github.com/user-attachments/assets/a1df99ea-096f-438b-b997-caa935f72010)\n",
        "\n",
        "출처: https://medium.com/@Suraj_Yadav/in-depth-knowledge-of-convolutional-neural-networks-b4bfff8145ab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXFJUE2-IorM"
      },
      "source": [
        "## Convolution Filter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKcOQzDqUfFJ"
      },
      "source": [
        "![image](https://github.com/user-attachments/assets/c1548757-cf97-4d55-ae81-e18b937a344a)\n",
        "\n",
        "![image](https://github.com/user-attachments/assets/081a29da-9095-4438-a58f-66607485631b)\n",
        "\n",
        "![image](https://github.com/user-attachments/assets/d7b4f327-d128-4da9-ac59-fe5f131acd5a)\n",
        "\n",
        "![image](https://github.com/user-attachments/assets/52666246-3017-4a8d-9b3d-6ea72099f2ae)\n",
        "\n",
        "![image](https://github.com/user-attachments/assets/34e38a4e-25d8-4ca0-acf7-aeb8b9d3ab47)\n",
        "\n",
        "![image](https://github.com/user-attachments/assets/8cd429d8-a56f-4aeb-be34-6a463731a77a)\n",
        "\n",
        "![image](https://github.com/user-attachments/assets/a15a033c-3f74-47c1-81c8-99ad75609f2a)\n",
        "\n",
        "![image](https://github.com/user-attachments/assets/8706cd10-4600-42d7-9445-5740a2ecd93d)\n",
        "\n",
        "![image](https://github.com/user-attachments/assets/71dd5e03-5794-4ff5-bcae-f92d71c6d537)\n",
        "\n",
        "![image](https://github.com/user-attachments/assets/971729ad-002e-4864-bd61-c3ac59c70ec9)\n",
        "\n",
        "![image](https://github.com/user-attachments/assets/f96372d8-a96d-45c4-bb34-d4aaf74314c3)\n",
        "\n",
        "\n",
        "출처: https://dev.to/sandeepbalachandran/machine-learning-convolution-with-color-images-2p41"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC_Md3JYV1i_"
      },
      "source": [
        "## CNN 예제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vDiiYWFIDc7",
        "outputId": "0b1bbe6d-4ea7-46e9-b4c5-f2b3f1b93e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 2., 0., 1.],\n",
            "         [0., 1., 0., 1., 0.],\n",
            "         [0., 1., 0., 0., 0.],\n",
            "         [1., 0., 1., 0., 2.],\n",
            "         [0., 1., 2., 2., 1.]],\n",
            "\n",
            "        [[0., 0., 1., 1., 2.],\n",
            "         [2., 2., 0., 1., 1.],\n",
            "         [0., 0., 0., 1., 0.],\n",
            "         [2., 0., 1., 0., 1.],\n",
            "         [1., 1., 2., 2., 0.]]])\n",
            "====================================================================================================\n",
            "tensor([[[[3., 1.],\n",
            "          [2., 3.]],\n",
            "\n",
            "         [[2., 4.],\n",
            "          [4., 3.]]],\n",
            "\n",
            "\n",
            "        [[[3., 2.],\n",
            "          [1., 4.]],\n",
            "\n",
            "         [[2., 4.],\n",
            "          [3., 2.]]],\n",
            "\n",
            "\n",
            "        [[[2., 2.],\n",
            "          [1., 4.]],\n",
            "\n",
            "         [[2., 4.],\n",
            "          [1., 1.]]]])\n",
            "tensor([1., 2., 3.])\n",
            "====================================================================================================\n",
            "tensor([[[ 1.,  1., 10., 12., 14., 11.],\n",
            "         [ 7., 18., 17., 19., 21., 12.],\n",
            "         [ 9., 17., 10.,  9., 14.,  3.],\n",
            "         [10., 12., 10., 11., 12.,  9.],\n",
            "         [13., 18., 24., 30., 22., 11.],\n",
            "         [ 5.,  8., 16., 21., 12.,  4.]],\n",
            "\n",
            "        [[ 2.,  2., 12.,  9., 13.,  9.],\n",
            "         [ 6., 16., 17., 20., 20., 12.],\n",
            "         [10., 20., 10., 10., 14.,  4.],\n",
            "         [10., 11., 11., 10., 14.,  7.],\n",
            "         [14., 18., 24., 27., 22., 11.],\n",
            "         [ 6., 10., 19., 24., 14.,  5.]],\n",
            "\n",
            "        [[ 3.,  3., 12.,  7., 10.,  6.],\n",
            "         [ 5., 11., 14., 18., 18., 10.],\n",
            "         [11., 21., 10., 10., 12.,  5.],\n",
            "         [ 9.,  8., 10.,  9., 14.,  6.],\n",
            "         [14., 15., 21., 21., 19., 10.],\n",
            "         [ 7., 11., 19., 23., 13.,  5.]]], grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "cnn1 = torch.nn.Conv2d(2, 3, kernel_size=2, stride=1, padding=1)\n",
        "\n",
        "cnn1.weight.data = torch.randint(low=1, high=5, size=(3, 2, 2, 2), dtype=torch.int).float()\n",
        "cnn1.bias.data = torch.tensor([1,2,3]).float()\n",
        "\n",
        "\n",
        "\n",
        "input = torch.randint(low=0, high=3, size=(2, 5, 5), dtype=torch.int).float()\n",
        "print(input)\n",
        "print(\"=\" * 100)\n",
        "print(cnn1.weight.data)\n",
        "print(cnn1.bias.data)\n",
        "print(\"=\" * 100)\n",
        "output = cnn1(input)\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-AcsbyqV8Ky",
        "outputId": "2803577b-4791-47d2-b9c6-8412614ef5ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape :  torch.Size([3, 32, 32])\n",
            "Output shape:  torch.Size([5, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "cnn1 = torch.nn.Conv2d(3, 5, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "\n",
        "input = torch.randint(low=0, high=3, size=(3, 32, 32), dtype=torch.int).float()\n",
        "print(\"Input shape : \", input.shape)\n",
        "output = cnn1(input)\n",
        "print(\"Output shape: \", output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu8zlytjdHAJ"
      },
      "source": [
        "## CNN의 장점"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWwTtp1PdTIP"
      },
      "source": [
        "Convolutional Layer(CNN 레이어)는 Fully Connected(FC) Layer에 비해 몇 가지 중요한 장점이 있습니다. 이러한 장점은 CNN이 특히 이미지 데이터에서 뛰어난 성능을 발휘하게 하는 요인입니다.\n",
        "\n",
        "### 1. 공간적 관계 유지\n",
        "> **FC Layer**: 모든 입력 뉴런이 모든 출력 뉴런에 연결되므로 입력의 공간적 구조가 무시됩니다. 따라서 FC Layer는 입력 데이터의 공간적 관계를 인식할 수 없습니다.\n",
        "\n",
        "> **Convolution Layer**: 입력 이미지의 작은 영역(리셉티브 필드)에 대해서만 필터를 적용합니다. 이로 인해 CNN은 인접 픽셀 간의 관계를 유지하고 학습할 수 있으며, 이미지의 공간적 패턴을 인식하는 데 유리합니다.\n",
        "\n",
        "### 2. 파라미터 수 감소\n",
        "> **FC Layer**: 입력과 출력의 모든 뉴런이 연결되므로, 입력 크기와 출력 크기에 따라 파라미터 수가 급격히 증가합니다.\n",
        "\n",
        "> **Convolution Layer**: 동일한 필터를 이미지 전체에 걸쳐 적용하므로, 파라미터 수가 크게 줄어듭니다. 이는 파라미터 공유(parameter sharing)로 인해 가능하며, 모델이 더 효율적이고 메모리 절약적입니다.\n",
        "\n",
        "### 3. 변환 불변성(Translation Invariance)\n",
        "> **FC Layer**: 입력 데이터의 위치 변화에 민감하여, 데이터의 이동이나 왜곡에 취약합니다.\n",
        "\n",
        "> **Convolution Layer**: 필터가 이미지의 모든 위치에 동일하게 적용되기 때문에, 객체의 위치 변화에 대해 비교적 강인합니다. 이는 이미지 내 객체의 위치가 약간 변해도 그 특징을 잘 인식할 수 있게 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXfui7CsZnDQ"
      },
      "source": [
        "# Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bkbxElsZzxC"
      },
      "source": [
        "![_2024_08_06_14_10_21_153-ezgif com-video-to-gif-converter](https://github.com/user-attachments/assets/a25f6253-c4d9-44e7-af42-9e7edc1c3c3f)\n",
        "\n",
        "출처: https://www.youtube.com/watch?v=pj9-rr1wDhM&t=336s\n",
        "\n",
        "![image](https://github.com/user-attachments/assets/4c516742-0a3b-4dbf-80bc-4feadd1534cf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3hbP4jtWW2z",
        "outputId": "537ad1de-9354-4011-f5ad-e15774c1bf9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 20,  30],\n",
            "         [112,  37]]])\n"
          ]
        }
      ],
      "source": [
        "input = torch.tensor([[[12 , 20 , 30, 0 ],\n",
        "                       [8  , 12 , 2 , 0 ],\n",
        "                       [34 , 70 , 37, 4 ],\n",
        "                       [112, 100, 25, 12]]])\n",
        "\n",
        "MaxPool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "Output = MaxPool(input)\n",
        "print(Output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
